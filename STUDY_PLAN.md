# 社交媒体爬虫 & 数据分析学习计划

> 目标：在 6-9 个月内，从零基础爬虫过渡到可以独立完成「社交媒体數據采集 + 基本分析」的项目，并在 GitHub 上形成可展示的作品集。

---

## 一、整体路线

1. Python 基础 + 爬虫入门  
2. 进阶爬虫：登录、分页、简单反爬  
3. 社交媒体数据抓取（优先 API，其次页面抓取）  
4. 数据清洗与分析（pandas + 可视化 + 文本分析）  
5. 打磨 1–2 个完整项目，作为作品集

每个阶段都至少产出 1 个小项目/仓库分支，而不是只看教程。

---

## 二、阶段规划

### 阶段 1（第 1–2 个月）：Python + 基础爬虫

**目标**：熟练掌握 Python 基本语法和 requests + BeautifulSoup 爬取静态页面的能力。

**需要掌握**：
- Python：
  - 变量、条件、循环、函数、模块、异常处理
  - 文件读写、虚拟环境（venv）
- HTTP 与网页基础：
  - URL、请求方法（GET/POST）、状态码
  - headers、cookies、User-Agent 的概念
- 爬虫库：
  - requests 发送请求、处理响应
  - BeautifulSoup / lxml 解析 HTML，使用 CSS 选择器 / XPath 提取数据
- 数据处理入门：
  - pandas：DataFrame 的创建、筛选、分组、导出 CSV

**练习/项目**：
1. 招聘信息爬虫 + 职位需求分析
   - 选一个公开招聘网站/板块（不需要登录的那种），抓取：职位名称、城市、薪资、发布日期、关键技能标签。
   - 使用 pandas 统计：
     - 哪些城市职位最多
     - 最常见的技能关键词 Top 10
   - 导出 CSV，并画一两张简单的柱状图/饼图。

2. 图书/电影信息爬虫 + 评分统计
   - 抓取某个公开榜单（比如电影 Top 榜）的标题、年份、评分、评论数等。
   - 统计：
     - 按年份分布
     - 评分区间分布（比如 8–9 分有多少部）。

> 建议：你现在的 Crawler_Practice 仓库可以新建一个 `jobs_analysis/` 或 `movies_analysis/` 目录，把这阶段的代码放进去。

---

### 阶段 2（第 3–5 个月）：进阶爬虫 + 反爬基础

**目标**：能够处理简单登录、分页、反爬策略，为之后社交媒体抓取打基础。

**需要掌握**：
- 会话保持：
  - requests.Session 维持 cookies
  - 模拟简单登录（不涉及复杂验证码）
- 分页与列表抓取模式：
  - 分析分页参数（page、offset、limit）
  - 封装通用的「翻页抓取」函数
- 简单反爬对抗：
  - 随机 User-Agent、合理设置 headers
  - 控制请求频率（time.sleep，或简单限流）
  - 使用免费代理的基本方法和风险意识
- JS 渲染页面：
  - 使用 Selenium 或 Playwright 打开需要 JS 渲染的页面
  - 找到元素 → 滚动 → 拿到 HTML/接口数据

**练习/项目**：
1. 登录后页面抓取小项目
   - 示例：需要登录才能访问的简单论坛/个人中心页面（不包含敏感信息）。
   - 实现：
     - 使用 Session + 表单登录
     - 抓取登录后才能看到的列表/信息

2. 具有简单反爬的网站抓取
   - 找一个会封 IP 或要求 headers 的站点：
     - 实现 User-Agent 轮换
     - 控制访问频率，避免被封

> 代码结构上，尝试把「请求层」「解析层」「存储层」分成不同模块/函数，为后面的社交媒体项目打好可维护性基础。

---

### 阶段 3（第 6–8 个月）：社交媒体数据抓取

**目标**：针对 1–2 个社交平台，能稳定、可控地采集帖子/评论等数据。

**学习重点**：
- 优先使用官方或公开 API：
  - 学习阅读 API 文档
  - 了解认证方式（API Key、OAuth 等）
  - 理解 rate limit（限流）规则
- 若无官方 API，再考虑页面抓取：
  - 分析网络请求（浏览器 DevTools 的 Network 面板）
  - 找到真实的 JSON 接口，避免直接从 HTML 抠文本
- 数据结构设计：
  - 统一的帖子字段：id、作者、发布时间、正文、点赞/评论/转发数、话题标签等
  - 统一的评论字段：评论 id、对应的帖子 id、评论者、时间、内容、点赞数

**练习/项目**（任选 1–2 个深入做）：
1. 话题热度追踪
   - 选一个社交平台（微博/Twitter/X 等）上的特定话题或关键词。
   - 周期性抓取（比如每小时/每天跑一次）：
     - 新帖子数量
     - 互动数据（点赞、转发、评论）
   - 目标：做出一条「话题热度随时间变化」的折线图。

2. 评论情绪分析
   - 选一个视频/帖子（比如 YouTube 视频、短视频平台内容）。
   - 抓取评论文本、时间、点赞数。
   - 使用中文或英文情感分析工具（例如现成情感字典或简单预训练模型接口）：
     - 统计正向/负向/中性评论比例
     - 结合时间，看情绪是否有明显变化。

3. 小型「社交媒体数据抓取工具」
   - 设计一个可配置脚本：
     - 通过配置文件/命令行参数指定：关键词、时间范围、平台
     - 自动抓取数据，存为 CSV/SQLite
   - 在 README 里说明使用方式和示例。

---

### 阶段 4（第 8–9 个月）：数据分析 + 作品集打磨

**目标**：把「能爬」升级成「能用数据讲故事」。

**需要掌握**：
- pandas 进阶：
  - groupby 聚合、多表 merge/join
  - 时间序列处理（设置时间索引、重采样、滑动平均）
- 可视化：
  - matplotlib / seaborn：折线图、柱状图、直方图、箱线图、热力图
  - 可选：plotly / pyecharts 做交互图
- 文本分析：
  - 中文分词（jieba 等）或英文 tokenization
  - 关键词提取（TF-IDF）
  - 简单情感分析（现成模型/服务即可）

**最终作品集建议**：至少做出 1–2 个「端到端」项目：
1. 从社交平台抓取数据（含采集脚本）
2. 数据清洗与预处理（Jupyter Notebook 或脚本）
3. 分析与可视化（图表 + 文字结论）
4. 在 README 中写清：
   - 背景与问题
   - 数据来源与字段说明
   - 分析步骤
   - 关键发现与结论

---

## 三、日/周学习节奏建议

- **工作日**：
  - 1–2 小时：看教程 + 做小练习（语法/库的用法）
  - 1 小时：在当前阶段项目里写代码、重构或补文档
- **周末**：
  - 整块 3–4 小时：
    - 集中实现一个功能（例如：分页抓取、登录、情感分析模块）
    - 写 README / Notebook 整理本周进度

保持「边学边做 + 每周都有 commit」，这样几个月后你的 GitHub 上就是一个完整的成长轨迹。

---

## 四、如何逐步升级当前仓库

以当前 `Crawler_Practice` 仓库为例，可以按阶段新建目录：

- `01_basic_requests_bs4/`：最简单的抓取练习（静态页面）
- `02_jobs_movie_projects/`：招聘 & 电影项目
- `03_login_and_antibot/`：登录、反爬相关练习
- `04_social_media_api/`：社交媒体 API/接口抓取
- `05_social_media_analysis/`：完整分析项目（Notebook + 图表）

每个目录都：
- 放源码
- 放示例数据（如 sample.csv）
- 写一个小 README，总结你学到了什么

---add study plan

## 五、下一步建议

1. 先完成阶段 1：在当前仓库新建一个「招聘信息爬虫 + 职位分析」或「电影评分分析」小项目目录。
2. 把本 STUDY_PLAN.md 提交到仓库，后续每完成一个阶段就在文档末尾追加「进度记录」。

这样你既有清晰路线，又能在简历/面试时给出非常具体、可展示的成长过程。
