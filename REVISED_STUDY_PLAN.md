# 社交媒体爬虫 & 数据分析进阶路线图

> **终极目标**：在 6-9 个月内，从零基础构建一套「社交媒体数据采集 + 深度分析」的作品集。
> **核心原则**：以项目为导向 (Project-Based Learning)，不仅要“写代码”，更要“讲故事”。

---

## 📂 建议仓库结构
为了让你的 GitHub 看起来专业，建议按照以下结构组织你的代码：

```text
WebCrawler/
├── 01_Basics_Static/          # 阶段 1：基础爬虫 (Requests + BS4)
├── 02_Advanced_Dynamic/       # 阶段 2：进阶爬虫 (Selenium/Playwright/登录)
├── 03_SocialMedia_API/        # 阶段 3：社交媒体 API & 规模化抓取
├── 04_Final_Analysis/         # 阶段 4：数据清洗、可视化与分析报告
├── data/                      # 存放抓取到的原始数据 (csv/json)
├── notebooks/                 # Jupyter Notebooks 用于探索性分析
├── requirements.txt           # 项目依赖
└── README.md                  # 项目总览
```

---

## 🚀 阶段一：基础夯实 (第 1–2 个月)
**主题：静态网页抓取与基础数据处理**

### ✅ 核心技能清单
- [ ] **Python 基础**: 列表/字典操作, 异常处理 (try-except), 文件读写 (csv/json)
- [ ] **HTTP 协议**: 理解 GET/POST, Headers, User-Agent, Status Codes
- [ ] **爬虫库**: `requests` 发送请求, `BeautifulSoup` 或 `lxml` 解析 HTML
- [ ] **数据工具**: `pandas` 基础 (DataFrame 创建, 筛选, 简单统计, 导出)

### 📚 推荐资源
*   **视频**: 莫烦 Python《Python 爬虫基础教程》 / B 站「爬虫教程」系列
*   **文章**: Requests 官方文档, Pandas 10分钟入门

### 🛠️ 实战项目 A：招聘/影评数据分析
**任务描述**: 抓取一个**无需登录**的公开网站 (如招聘网、豆瓣 Top250、IMDB)。
**产出要求**:
1.  **脚本**: `crawl_data.py` - 自动翻页抓取至少 5-10 页数据。
2.  **数据**: 保存为 `clean_data.csv`，包含关键字段 (如: 职位名, 薪资, 地区, 技能标签)。
3.  **分析**: 使用 pandas 统计 Top 10 热门城市或高频技能词。
4.  **图表**: 生成 1 张简单的柱状图或饼图。

---

## 🛡️ 阶段二：攻克难关 (第 3–5 个月)
**主题：动态网页、模拟登录与反爬对抗**

### ✅ 核心技能清单
- [ ] **会话管理**: 使用 `requests.Session` 维持 Cookie 实现登录状态
- [ ] **动态渲染**: 使用 `Playwright` 或 `Selenium` 处理 JavaScript 生成的页面
- [ ] **反爬对抗**: 随机 User-Agent, 请求频率控制 (Sleep), 代理 IP 概念
- [ ] **网络分析**: 熟练使用浏览器 DevTools (Network 面板) 分析 XHR/Fetch 请求

### 📚 推荐资源
*   **工具**: Playwright for Python 官方文档
*   **教程**: Scrapfly 博客 (关于 Playwright 和反爬策略的文章)

### 🛠️ 实战项目 B：模拟登录与动态抓取
**任务描述**: 针对一个**需要登录**或**内容动态加载** (瀑布流/点击加载) 的网站。
**产出要求**:
1.  **脚本**: `dynamic_crawler.py` - 实现了自动登录 (接收 Cookie) 或 自动化浏览器操作。
2.  **策略**: 代码中包含随机等待时间 (`random.sleep`) 和 User-Agent 轮换。
3.  **数据**: 抓取个人收藏列表或动态加载的评论区内容。

---

## 🐦 阶段三：社交媒体专项 (第 6–8 个月)
**主题：API 利用与特定平台深耕**

### ✅ 核心技能清单
- [ ] **API 交互**: 申请并使用 API Key, 理解 OAuth 认证, 处理 Rate Limit (限流)
- [ ] **逆向分析**: 如果无官方 API，分析私有 API (JSON 接口) 结构
- [ ] **数据建模**: 设计合理的 JSON 结构存储 帖子(Post) 与 评论(Comment) 的层级关系
- [ ] **自动化**: 编写定时脚本 (Cron job 或 简单循环) 追踪数据变化

### 📚 推荐资源
*   **文档**: Twitter/X API 文档, YouTube Data API 文档
*   **文章**: "Reverse engineering private APIs" 相关教程

### 🛠️ 实战项目 C：舆情热度追踪器
**任务描述**: 选取一个话题 (如 "AI技术", "新上映电影")，在 Twitter/微博/B站/YouTube 追踪其热度。
**产出要求**:
1.  **监控**: 编写脚本每天/每小时抓取一次该话题下的新帖数量及互动量 (点赞/转发)。
2.  **存储**: 将不同时间点的数据追加保存 (Append) 到文件或简单数据库 (SQLite)。
3.  **展示**: 绘制一条“热度随时间变化”的折线图。

---

## 📊 阶段四：数据叙事 (第 8–9 个月)
**主题：深度分析、NLP 与 完整报告**

### ✅ 核心技能清单
- [ ] **Pandas 进阶**: GroupBy 复杂聚合, 多表关联 (Merge), 时间序列处理
- [ ] **可视化**: `matplotlib` / `seaborn` / `plotly` 绘制复杂图表 (热力图, 箱线图)
- [ ] **文本挖掘**: 结巴分词 (Jieba), 词云生成, 基础情感分析 (SnowNLP/TextBlob)
- [ ] **报告撰写**: 使用 Jupyter Notebook 撰写图文并茂的分析报告

### 🛠️ 实战项目 D：社交媒体洞察报告 (毕业设计)
**任务描述**: 将阶段三抓取的数据进行深度挖掘，回答“为什么”。
**产出要求**:
1.  **完整 Notebook**: 包含 数据加载 -> 清洗 -> 探索 -> 建模/分析 -> 结论。
2.  **情感分析**: 分析评论区的情绪倾向 (正向 vs 负向) 占比。
3.  **结论**: 用数据回答问题，例如“哪类标题更容易获得高赞？”、“用户对该话题的情绪随时间如何变化？”。
4.  **README**: 一个漂亮的 GitHub 首页，介绍项目背景、技术栈和核心发现。
