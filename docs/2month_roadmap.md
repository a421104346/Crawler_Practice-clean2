# 2ä¸ªæœˆFastAPI+Reactå…¨æ ˆå­¦ä¹ è®¡åˆ’ï¼ˆç”Ÿäº§çº§çˆ¬è™«ç³»ç»Ÿï¼‰

## æ ¸å¿ƒå†³ç­–ä¸ç†ç”±

### ä¸ºä»€ä¹ˆé€‰FastAPIï¼Ÿ

**å…³é”®æ•°æ®ï¼ˆ2025å¹´ï¼‰ï¼š**
- FastAPIé‡‡ç”¨ç‡å¢é•¿40%ï¼ˆä»29% â†’ 38%ï¼‰[æ¥æºï¼šæœ€æ–°è¡Œä¸šæ•°æ®]
- æ€§èƒ½ï¼š15,000-20,000 req/s vs Flaskçš„2,000-3,000 req/s
- ä½ çš„çˆ¬è™«é¡¹ç›®æ˜¯I/Oå¯†é›†å‹ â†’ FastAPIå®Œç¾é€‚é…
- ä¸€æ¬¡å­¦ä¹ ï¼ŒèŒä¸šç”Ÿæ¶¯å—ç”¨

**é¿å…çš„é™·é˜±ï¼š**
- âŒ å…ˆå­¦Flaskï¼Œ3ä¸ªæœˆåå‘ç°æ€§èƒ½/åŠŸèƒ½ä¸è¶³ï¼Œé‡æ–°å­¦FastAPI
- âœ… ç›´æ¥å­¦FastAPIï¼Œä¸ç”¨å›å¤´

### æ¶æ„æœ€ç»ˆæ–¹æ¡ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React å‰ç«¯                        â”‚  (TypeScript)
â”‚   â”œâ”€ çˆ¬è™«æ§åˆ¶é¢æ¿                   â”‚
â”‚   â”œâ”€ å®æ—¶è¿›åº¦/ç»Ÿè®¡                  â”‚
â”‚   â”œâ”€ ç»“æœå±•ç¤ºå’Œä¸‹è½½                 â”‚
â”‚   â””â”€ ä»»åŠ¡å†å²ç®¡ç†                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP + WebSocket
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI åç«¯ (Async)              â”‚
â”‚   â”œâ”€ RESTful API                    â”‚
â”‚   â”œâ”€ WebSocket å®æ—¶æ¨é€             â”‚
â”‚   â”œâ”€ JWT è®¤è¯                       â”‚
â”‚   â”œâ”€ åå°ä»»åŠ¡ç®¡ç†                   â”‚
â”‚   â””â”€ æ•°æ®éªŒè¯ (Pydantic)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ ¸å¿ƒçˆ¬è™«åº“ï¼ˆä½ ç°æœ‰çš„ï¼‰             â”‚
â”‚   â”œâ”€ BaseCrawler                    â”‚
â”‚   â”œâ”€ YahooCrawler                   â”‚
â”‚   â”œâ”€ MoviesCrawler                  â”‚
â”‚   â”œâ”€ JobsCrawler                    â”‚
â”‚   â””â”€ ...                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ å¼‚æ­¥è°ƒç”¨
       â–¼
  Celery + Redis (å¯é€‰ç¬¬ä¸‰é˜¶æ®µå‡çº§)
```

---

## ç¬¬1é˜¶æ®µï¼šFastAPI + å¼‚æ­¥ç¼–ç¨‹åŸºç¡€ï¼ˆ2å‘¨ï¼‰

### Week 1ï¼šPythonå¼‚æ­¥ç¼–ç¨‹ + FastAPIå…¥é—¨

**Day 1-2ï¼šPython Async/Await æ·±åº¦ç†è§£**

æ ¸å¿ƒæ¦‚å¿µå­¦ä¹ ï¼ˆä¸åªæ˜¯è¡¨é¢å­¦ä¹ ï¼‰ï¼š
```python
# 1. ç†è§£äº‹ä»¶å¾ªç¯
import asyncio

async def task_1():
    print("Task 1 start")
    await asyncio.sleep(2)  # æ¨¡æ‹ŸI/Oç­‰å¾…
    print("Task 1 done")
    return "Result 1"

async def task_2():
    print("Task 2 start")
    await asyncio.sleep(1)
    print("Task 2 done")
    return "Result 2"

# å¹¶å‘æ‰§è¡Œï¼ˆä¸æ˜¯å¹¶è¡Œï¼ï¼‰
async def main():
    # ä¸¤ä¸ªä»»åŠ¡å¹¶å‘è¿è¡Œï¼ˆäº¤æ›¿æ‰§è¡Œï¼‰
    result1, result2 = await asyncio.gather(task_1(), task_2())
    print(f"Results: {result1}, {result2}")
    # æ€»è€—æ—¶ ~2ç§’ï¼ˆè€Œé3ç§’ï¼‰

asyncio.run(main())

# å…³é”®ç‚¹ï¼šTask 1å’ŒTask 2äº¤æ›¿æ‰§è¡Œï¼Œå½“Task 1ç­‰å¾…æ—¶Task 2è¿è¡Œ
```

å­¦ä¹ ææ–™ï¼š
- å®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.python.org/3/library/asyncio.html
- é‡ç‚¹ï¼šäº‹ä»¶å¾ªç¯ã€awaitã€gatherã€create_taskã€as_completed

ç»ƒä¹ 1ï¼šå†™ä¸€ä¸ªèƒ½åŒæ—¶çˆ¬5ä¸ªè‚¡ç¥¨çš„å¼‚æ­¥å‡½æ•°
```python
import asyncio
import httpx  # å¼‚æ­¥HTTPåº“

async def fetch_stock(symbol: str):
    """å¼‚æ­¥çˆ¬å–å•ä¸ªè‚¡ç¥¨"""
    async with httpx.AsyncClient() as client:
        response = await client.get(f"https://api.example.com/stock/{symbol}")
        return response.json()

async def fetch_multiple_stocks(symbols: list):
    """å¹¶å‘çˆ¬å–å¤šä¸ªè‚¡ç¥¨"""
    tasks = [fetch_stock(symbol) for symbol in symbols]
    results = await asyncio.gather(*tasks)
    return results

# æµ‹è¯•
asyncio.run(fetch_multiple_stocks(['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']))
```

**Day 3-4ï¼šFastAPIåŸºç¡€**

å®‰è£…å’Œå¿«é€Ÿå¼€å§‹ï¼š
```bash
pip install fastapi uvicorn pydantic httpx
```

ç¬¬ä¸€ä¸ªFastAPIåº”ç”¨ï¼š
```python
# main.py
from fastapi import FastAPI
from pydantic import BaseModel
from typing import Optional
import asyncio

app = FastAPI(title="çˆ¬è™«API")

# 1. å®šä¹‰æ•°æ®æ¨¡å‹ï¼ˆè‡ªåŠ¨éªŒè¯å’Œæ–‡æ¡£ï¼‰
class CrawlerRequest(BaseModel):
    crawler_type: str
    symbol: Optional[str] = None
    page: Optional[int] = 1
    
    class Config:
        example = {
            "crawler_type": "yahoo",
            "symbol": "AAPL"
        }

class CrawlerResponse(BaseModel):
    task_id: str
    status: str
    message: str

# 2. å®šä¹‰è·¯ç”±
@app.get("/")
async def root():
    """æ ¹è·¯ç”±"""
    return {"message": "çˆ¬è™«ç®¡ç†ç³»ç»Ÿ"}

@app.get("/crawlers")
async def list_crawlers():
    """åˆ—å‡ºæ‰€æœ‰çˆ¬è™«"""
    return {
        "crawlers": ["yahoo", "movies", "jobs", "douban", "weather", "news"]
    }

@app.post("/api/crawlers/{crawler_type}/run")
async def run_crawler(crawler_type: str, request: CrawlerRequest):
    """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
    if crawler_type not in ["yahoo", "movies", "jobs"]:
        return {"error": "Unknown crawler"}, 404
    
    return CrawlerResponse(
        task_id="uuid-123",
        status="started",
        message=f"Started {crawler_type} crawler"
    )

@app.get("/docs")  # è‡ªåŠ¨ç”Ÿæˆçš„APIæ–‡æ¡£

# è¿è¡Œï¼šuvicorn main:app --reload
```

å…³é”®ç‰¹æ€§ç†è§£ï¼š
- âœ… **è‡ªåŠ¨éªŒè¯** - Pydanticæ¨¡å‹è‡ªåŠ¨éªŒè¯è¾“å…¥
- âœ… **è‡ªåŠ¨æ–‡æ¡£** - è®¿é—® http://localhost:8000/docs
- âœ… **ç±»å‹æç¤º** - IDEè‡ªåŠ¨è¡¥å…¨ï¼Œç±»å‹æ£€æŸ¥
- âœ… **å¼‚æ­¥åŸç”Ÿ** - æ‰€æœ‰è·¯ç”±å¤©ç”Ÿæ”¯æŒasync

ç»ƒä¹ 2ï¼šåˆ›å»ºä¸€ä¸ªèƒ½æ¥æ”¶è‚¡ç¥¨ä»£ç çš„FastAPIç«¯ç‚¹
```python
from fastapi import HTTPException

@app.post("/api/analyze/{symbol}")
async def analyze_stock(symbol: str, days: int = 30):
    """åˆ†æè‚¡ç¥¨"""
    if not symbol or len(symbol) > 5:
        raise HTTPException(status_code=400, detail="Invalid symbol")
    
    # æ¨¡æ‹Ÿå¼‚æ­¥åˆ†æ
    await asyncio.sleep(0.5)
    
    return {
        "symbol": symbol,
        "days": days,
        "trend": "up",
        "volatility": 0.15
    }
```

**Day 5-6ï¼šFastAPI + ä½ çš„çˆ¬è™«æ•´åˆ**

åˆ›å»ºçˆ¬è™«åŒ…è£…å™¨ï¼š
```python
# backend/app.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
import asyncio
from typing import Dict
import uuid
from datetime import datetime

# å¯¼å…¥ä½ ç°æœ‰çš„çˆ¬è™«
import sys
sys.path.insert(0, '../')
from core.base_crawler import BaseCrawler
from crawlers.yahoo import YahooCrawler
from crawlers.movies import MoviesCrawler

app = FastAPI(title="çˆ¬è™«ç®¡ç†ç³»ç»Ÿ")

# ä»»åŠ¡å­˜å‚¨ï¼ˆç¬¬äºŒé˜¶æ®µå‡çº§åˆ°æ•°æ®åº“ï¼‰
tasks_db: Dict = {}

@app.post("/api/crawlers/{crawler_type}/run")
async def run_crawler(
    crawler_type: str,
    background_tasks: BackgroundTasks,
    request: dict
):
    """å¯åŠ¨çˆ¬è™«ï¼ˆåå°è¿è¡Œï¼‰"""
    
    task_id = str(uuid.uuid4())
    
    # åå°ä»»åŠ¡
    background_tasks.add_task(
        execute_crawler,
        task_id=task_id,
        crawler_type=crawler_type,
        params=request
    )
    
    tasks_db[task_id] = {
        "status": "pending",
        "crawler": crawler_type,
        "created_at": datetime.now().isoformat()
    }
    
    return {"task_id": task_id, "status": "queued"}

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in tasks_db:
        raise HTTPException(status_code=404, detail="Task not found")
    
    return tasks_db[task_id]

async def execute_crawler(task_id: str, crawler_type: str, params: dict):
    """åå°çˆ¬è™«æ‰§è¡Œ"""
    try:
        tasks_db[task_id]["status"] = "running"
        
        if crawler_type == "yahoo":
            crawler = YahooCrawler(**params)
        elif crawler_type == "movies":
            crawler = MoviesCrawler(**params)
        else:
            raise ValueError("Unknown crawler")
        
        # è¿è¡Œçˆ¬è™«ï¼ˆåŒæ­¥è½¬å¼‚æ­¥ï¼‰
        result = await asyncio.to_thread(crawler.run)
        
        tasks_db[task_id] = {
            "status": "completed",
            "result": result,
            "completed_at": datetime.now().isoformat()
        }
    except Exception as e:
        tasks_db[task_id] = {
            "status": "failed",
            "error": str(e)
        }

# è¿è¡Œ
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Day 7-14ï¼šå·©å›º + é¡¹ç›®æ•´åˆ**

- åˆ›å»ºFastAPIé¡¹ç›®ç›®å½•ç»“æ„
- é›†æˆæ‰€æœ‰6ä¸ªçˆ¬è™«
- æ·»åŠ é”™è¯¯å¤„ç†ã€æ—¥å¿—
- ç¼–å†™å•å…ƒæµ‹è¯•

---

### Week 2ï¼šWebSocketå®æ—¶é€šä¿¡ + è®¤è¯

**Day 8-9ï¼šWebSocketå®æ—¶è¿›åº¦æ¨é€**

ä¸ºä»€ä¹ˆéœ€è¦WebSocketï¼Ÿ
- HTTPè½®è¯¢ï¼šæ¯500mså‘ä¸€æ¬¡è¯·æ±‚ï¼Œæµªè´¹å¸¦å®½
- WebSocketï¼šå»ºç«‹æŒä¹…è¿æ¥ï¼Œå®æ—¶æ¨é€è¿›åº¦

```python
from fastapi import WebSocket

# å­˜å‚¨æ´»è·ƒè¿æ¥
class ConnectionManager:
    def __init__(self):
        self.active_connections: list[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    
    async def broadcast(self, message: dict):
        """å¹¿æ’­è¿›åº¦åˆ°æ‰€æœ‰è¿æ¥"""
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except:
                pass

manager = ConnectionManager()

@app.websocket("/ws/crawler/{task_id}")
async def websocket_endpoint(websocket: WebSocket, task_id: str):
    """WebSocketç«¯ç‚¹ï¼šå®æ—¶æ¨é€çˆ¬è™«è¿›åº¦"""
    await manager.connect(websocket)
    try:
        while True:
            # æ¥æ”¶å‰ç«¯æ¶ˆæ¯ï¼ˆä¾‹å¦‚æš‚åœ/å–æ¶ˆï¼‰
            data = await websocket.receive_text()
            
            if task_id in tasks_db:
                # æ¨é€å½“å‰çŠ¶æ€
                await websocket.send_json(tasks_db[task_id])
            
            await asyncio.sleep(0.2)
    except:
        manager.disconnect(websocket)

# çˆ¬è™«æ‰§è¡Œæ—¶ï¼Œå®æ—¶å¹¿æ’­è¿›åº¦
async def execute_crawler_with_progress(task_id: str, crawler_type: str):
    """æ”¹è¿›çš„çˆ¬è™«æ‰§è¡Œï¼ˆå¸¦è¿›åº¦æ¨é€ï¼‰"""
    try:
        crawler = get_crawler(crawler_type)
        
        # å‡è®¾çˆ¬è™«æ”¯æŒè¿›åº¦å›è°ƒ
        def progress_callback(current, total, message=""):
            progress = int((current / total) * 100)
            
            # å¹¿æ’­è¿›åº¦
            asyncio.create_task(
                broadcast_to_clients(task_id, {
                    "status": "running",
                    "progress": progress,
                    "message": message
                })
            )
        
        crawler.set_progress_callback(progress_callback)
        result = await asyncio.to_thread(crawler.run)
        
        await broadcast_to_clients(task_id, {
            "status": "completed",
            "result": result,
            "progress": 100
        })
    except Exception as e:
        await broadcast_to_clients(task_id, {
            "status": "failed",
            "error": str(e)
        })

async def broadcast_to_clients(task_id: str, message: dict):
    """å¹¿æ’­è¿›åº¦ç»™è®¢é˜…è¯¥ä»»åŠ¡çš„æ‰€æœ‰å®¢æˆ·ç«¯"""
    for connection in manager.active_connections:
        await connection.send_json({
            "task_id": task_id,
            **message
        })
```

**Day 10-11ï¼šJWTè®¤è¯ä¸æˆæƒ**

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthCredentials
from datetime import datetime, timedelta
import jwt

# é…ç½®
SECRET_KEY = "your-secret-key-change-in-production"
ALGORITHM = "HS256"

security = HTTPBearer()

def create_access_token(data: dict, expires_delta: timedelta = None):
    """ç”ŸæˆJWT token"""
    to_encode = data.copy()
    
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(hours=24)
    
    to_encode.update({"exp": expire})
    
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def get_current_user(credentials: HTTPAuthCredentials = Depends(security)):
    """éªŒè¯JWT token"""
    token = credentials.credentials
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401, detail="Invalid token")
        return user_id
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")

# ç™»å½•ç«¯ç‚¹
@app.post("/api/login")
async def login(username: str, password: str):
    """ç™»å½•è·å–token"""
    # ç®€å•éªŒè¯ï¼ˆå®é™…åº”ç”¨åº”ä½¿ç”¨æ•°æ®åº“ï¼‰
    if username == "admin" and password == "password":
        token = create_access_token({"sub": username})
        return {"access_token": token, "token_type": "bearer"}
    else:
        raise HTTPException(status_code=401, detail="Invalid credentials")

# ä¿æŠ¤çš„ç«¯ç‚¹
@app.get("/api/my-tasks")
async def get_my_tasks(user_id: str = Depends(get_current_user)):
    """åªæœ‰è®¤è¯ç”¨æˆ·æ‰èƒ½è®¿é—®"""
    # è¿”å›è¯¥ç”¨æˆ·çš„ä»»åŠ¡
    return {"user_id": user_id, "tasks": []}
```

**Day 12-14ï¼šé›†æˆå’Œæµ‹è¯•**

```python
# tests/test_api.py
import pytest
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

def test_read_crawlers():
    response = client.get("/crawlers")
    assert response.status_code == 200
    assert "yahoo" in response.json()["crawlers"]

def test_run_crawler():
    response = client.post(
        "/api/crawlers/yahoo/run",
        json={"symbol": "AAPL"}
    )
    assert response.status_code == 200
    assert "task_id" in response.json()

def test_get_task_status():
    # å…ˆå¯åŠ¨ä»»åŠ¡
    run_response = client.post("/api/crawlers/yahoo/run", json={})
    task_id = run_response.json()["task_id"]
    
    # è·å–çŠ¶æ€
    status_response = client.get(f"/api/tasks/{task_id}")
    assert status_response.status_code == 200

def test_invalid_crawler():
    response = client.post(
        "/api/crawlers/invalid/run",
        json={}
    )
    assert response.status_code == 404

# è¿è¡Œæµ‹è¯•
# pytest tests/test_api.py -v
```

---

## ç¬¬2é˜¶æ®µï¼šç”Ÿäº§çº§éƒ¨ç½² + æ•°æ®åº“é›†æˆï¼ˆ2å‘¨ï¼‰

### Week 3ï¼šæ•°æ®åº“ + ä»»åŠ¡é˜Ÿåˆ—

**Day 15-16ï¼šSQLAlchemy + SQLite**

```python
# backend/database.py
from sqlalchemy import create_engine, Column, String, Integer, DateTime, Text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from datetime import datetime

DATABASE_URL = "sqlite:///./crawler_tasks.db"

engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(bind=engine)
Base = declarative_base()

class TaskModel(Base):
    __tablename__ = "tasks"
    
    id = Column(String, primary_key=True)
    crawler_type = Column(String)
    status = Column(String)  # pending, running, completed, failed
    progress = Column(Integer, default=0)
    result = Column(Text)
    error = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    completed_at = Column(DateTime)

Base.metadata.create_all(bind=engine)

# ä½¿ç”¨ç¤ºä¾‹
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

# åœ¨è·¯ç”±ä¸­ä½¿ç”¨
from fastapi import Depends
from sqlalchemy.orm import Session

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str, db: Session = Depends(get_db)):
    task = db.query(TaskModel).filter(TaskModel.id == task_id).first()
    if not task:
        raise HTTPException(status_code=404)
    return {
        "task_id": task.id,
        "status": task.status,
        "progress": task.progress
    }
```

**Day 17-18ï¼šCelery + Redisï¼ˆå¯é€‰ä½†æ¨èï¼‰**

ä¸ºä»€ä¹ˆéœ€è¦ä»»åŠ¡é˜Ÿåˆ—ï¼Ÿ
- å¤„ç†é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡
- æ”¯æŒåˆ†å¸ƒå¼å¤„ç†
- ä»»åŠ¡æŒä¹…åŒ–å’Œé‡è¯•

```python
# backend/celery_app.py
from celery import Celery
import time

celery_app = Celery(
    "crawler_system",
    broker="redis://localhost:6379/0",
    backend="redis://localhost:6379/0"
)

@celery_app.task
def run_crawler_task(task_id: str, crawler_type: str, params: dict):
    """Celeryä»»åŠ¡ï¼šåå°è¿è¡Œçˆ¬è™«"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        update_task_status(task_id, "running", 0)
        
        crawler = get_crawler(crawler_type)
        
        # æ¨¡æ‹Ÿè¿›åº¦
        for i in range(1, 101):
            time.sleep(0.1)  # æ¨¡æ‹Ÿå·¥ä½œ
            update_task_status(task_id, "running", i)
        
        result = crawler.run()
        update_task_status(task_id, "completed", 100, result=result)
    except Exception as e:
        update_task_status(task_id, "failed", error=str(e))

# åœ¨FastAPIä¸­ä½¿ç”¨
@app.post("/api/crawlers/{crawler_type}/run")
async def run_crawler(crawler_type: str, db: Session = Depends(get_db)):
    task_id = str(uuid.uuid4())
    
    # åˆ›å»ºæ•°æ®åº“è®°å½•
    task = TaskModel(id=task_id, crawler_type=crawler_type, status="pending")
    db.add(task)
    db.commit()
    
    # æäº¤åˆ°Celeryé˜Ÿåˆ—
    run_crawler_task.delay(task_id, crawler_type, {})
    
    return {"task_id": task_id}

# å¯åŠ¨Celery worker
# celery -A backend.celery_app worker --loglevel=info
```

å®‰è£…ï¼š
```bash
pip install celery redis sqlalchemy
```

**Day 19-21ï¼šç”Ÿäº§éƒ¨ç½²é…ç½®**

```dockerfile
# backend/Dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# ç”Ÿäº§éƒ¨ç½²å‘½ä»¤
CMD ["gunicorn", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "main:app", "--bind", "0.0.0.0:8000"]
```

```bash
# requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
sqlalchemy==2.0.23
celery==5.3.4
redis==5.0.1
gunicorn==21.2.0
pytest==7.4.3
python-jose==3.3.0
httpx==0.25.2
```

```yaml
# docker-compose.yml
version: '3.8'
services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - redis
      - postgres
    environment:
      DATABASE_URL: postgresql://user:password@postgres:5432/crawler_db
      REDIS_URL: redis://redis:6379/0
  
  celery:
    build: ./backend
    command: celery -A celery_app worker --loglevel=info
    depends_on:
      - redis
      - postgres
    environment:
      DATABASE_URL: postgresql://user:password@postgres:5432/crawler_db
      REDIS_URL: redis://redis:6379/0
  
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
  
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: crawler_db
    ports:
      - "5432:5432"
```

---

### Week 4ï¼šReactå‰ç«¯å¼€å‘

**Day 22-23ï¼šReact + TypeScriptåŸºç¡€**

```tsx
// frontend/src/App.tsx
import React, { useState, useEffect } from 'react';
import './App.css';

interface Task {
  task_id: string;
  status: 'pending' | 'running' | 'completed' | 'failed';
  progress: number;
  crawler: string;
}

const App: React.FC = () => {
  const [tasks, setTasks] = useState<Task[]>([]);
  const [selectedCrawler, setSelectedCrawler] = useState('yahoo');
  const [loading, setLoading] = useState(false);

  // å¯åŠ¨çˆ¬è™«
  const handleRunCrawler = async () => {
    setLoading(true);
    try {
      const response = await fetch('/api/crawlers/yahoo/run', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ symbol: 'AAPL' })
      });
      const data = await response.json();
      
      // è¿æ¥WebSocket
      connectWebSocket(data.task_id);
    } catch (error) {
      console.error('Error:', error);
    }
    setLoading(false);
  };

  // WebSocketè¿æ¥
  const connectWebSocket = (taskId: string) => {
    const ws = new WebSocket(`ws://localhost:8000/ws/crawler/${taskId}`);
    
    ws.onmessage = (event) => {
      const message = JSON.parse(event.data);
      setTasks(prev => {
        const existing = prev.find(t => t.task_id === taskId);
        if (existing) {
          return prev.map(t => 
            t.task_id === taskId ? { ...t, ...message } : t
          );
        }
        return [...prev, { task_id: taskId, ...message }];
      });
    };
  };

  return (
    <div className="app">
      <h1>ğŸ•·ï¸ çˆ¬è™«ç®¡ç†ç³»ç»Ÿ</h1>
      
      <div className="control-panel">
        <select 
          value={selectedCrawler}
          onChange={(e) => setSelectedCrawler(e.target.value)}
        >
          <option>yahoo</option>
          <option>movies</option>
          <option>jobs</option>
        </select>
        
        <button onClick={handleRunCrawler} disabled={loading}>
          {loading ? 'å¯åŠ¨ä¸­...' : 'â–¶ï¸ å¼€å§‹çˆ¬å–'}
        </button>
      </div>

      <div className="tasks-container">
        {tasks.map(task => (
          <TaskCard key={task.task_id} task={task} />
        ))}
      </div>
    </div>
  );
};

interface TaskCardProps {
  task: Task;
}

const TaskCard: React.FC<TaskCardProps> = ({ task }) => {
  return (
    <div className={`task-card task-${task.status}`}>
      <h3>{task.crawler} - {task.task_id.substring(0, 8)}</h3>
      <div className="progress-bar">
        <div className="progress-fill" style={{ width: `${task.progress}%` }}></div>
      </div>
      <p>{task.progress}% - {task.status}</p>
    </div>
  );
};

export default App;
```

```css
/* frontend/src/App.css */
.app {
  max-width: 1200px;
  margin: 0 auto;
  padding: 20px;
  font-family: -apple-system, sans-serif;
}

.control-panel {
  display: flex;
  gap: 10px;
  margin: 20px 0;
}

.control-panel select,
.control-panel button {
  padding: 10px 20px;
  border: 1px solid #ddd;
  border-radius: 6px;
  font-size: 16px;
}

.control-panel button {
  background: #007bff;
  color: white;
  cursor: pointer;
  transition: background 0.3s;
}

.control-panel button:hover {
  background: #0056b3;
}

.tasks-container {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
  gap: 20px;
  margin-top: 20px;
}

.task-card {
  border: 1px solid #ddd;
  border-radius: 8px;
  padding: 20px;
  background: white;
}

.task-card.task-completed {
  border-color: #28a745;
  background: #f0fff4;
}

.task-card.task-failed {
  border-color: #dc3545;
  background: #fff5f5;
}

.progress-bar {
  width: 100%;
  height: 8px;
  background: #f0f0f0;
  border-radius: 4px;
  overflow: hidden;
  margin: 10px 0;
}

.progress-fill {
  height: 100%;
  background: linear-gradient(90deg, #007bff, #0056b3);
  transition: width 0.3s;
}
```

**Day 24-25ï¼šReacté«˜çº§åŠŸèƒ½**

```tsx
// frontend/src/hooks/useCrawler.ts
import { useState, useCallback } from 'react';

export const useCrawler = () => {
  const [tasks, setTasks] = useState([]);
  const [loading, setLoading] = useState(false);

  const runCrawler = useCallback(async (crawlerType: string, params: any) => {
    setLoading(true);
    try {
      const response = await fetch(`/api/crawlers/${crawlerType}/run`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(params)
      });
      
      const data = await response.json();
      const taskId = data.task_id;

      // è®¢é˜…WebSocket
      subscribeToTask(taskId);

      return taskId;
    } finally {
      setLoading(false);
    }
  }, []);

  const subscribeToTask = (taskId: string) => {
    const ws = new WebSocket(`ws://localhost:8000/ws/crawler/${taskId}`);
    
    ws.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setTasks(prev => {
        const existing = prev.find((t: any) => t.task_id === taskId);
        if (existing) {
          return prev.map((t: any) => 
            t.task_id === taskId ? { ...t, ...data } : t
          );
        }
        return [...prev, { task_id: taskId, ...data }];
      });
    };
  };

  return { tasks, loading, runCrawler };
};

// frontend/src/components/CrawlerForm.tsx
import React, { useState } from 'react';
import { useCrawler } from '../hooks/useCrawler';

interface CrawlerFormProps {
  crawlerType: string;
}

const CrawlerForm: React.FC<CrawlerFormProps> = ({ crawlerType }) => {
  const { runCrawler, loading } = useCrawler();
  const [params, setParams] = useState({});

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    await runCrawler(crawlerType, params);
  };

  return (
    <form onSubmit={handleSubmit}>
      {crawlerType === 'yahoo' && (
        <input
          type="text"
          placeholder="è¾“å…¥è‚¡ç¥¨ä»£ç "
          onChange={(e) => setParams({ ...params, symbol: e.target.value })}
        />
      )}
      
      <button type="submit" disabled={loading}>
        {loading ? 'å¯åŠ¨ä¸­...' : 'å¯åŠ¨çˆ¬è™«'}
      </button>
    </form>
  );
};

export default CrawlerForm;
```

**Day 26-28ï¼šé›†æˆå’Œä¼˜åŒ–**

- Redux / Zustand çŠ¶æ€ç®¡ç†
- é”™è¯¯å¤„ç†å’Œé‡è¯•é€»è¾‘
- ç»“æœå¯¼å‡ºåŠŸèƒ½
- æ·±è‰²æ¨¡å¼æ”¯æŒ

---

## ç¬¬3é˜¶æ®µï¼šçˆ¬è™«é¡¹ç›®é›†æˆï¼ˆ1å‘¨ï¼‰

**Day 29-32ï¼šå®Œæ•´é›†æˆ**

```
Crawler_Practice/
â”œâ”€â”€ backend/                     # FastAPIåç«¯
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ crawlers.py
â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â””â”€â”€ auth.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ crawler_service.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/                    # Reactå‰ç«¯
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ core/                        # ä½ ç°æœ‰çš„çˆ¬è™«æ ¸å¿ƒåº“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_crawler.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ crawlers/                    # ä½ ç°æœ‰çš„æ‰€æœ‰çˆ¬è™«
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ yahoo.py
â”‚   â”œâ”€â”€ movies.py
â”‚   â”œâ”€â”€ jobs.py
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ outputs/                     # æ•°æ®è¾“å‡º
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

æ•´åˆç¤ºä¾‹ï¼š
```python
# backend/services/crawler_service.py
from typing import Dict, Any
import sys
sys.path.insert(0, '../../')

from crawlers.yahoo import YahooCrawler
from crawlers.movies import MoviesCrawler
from crawlers.jobs import JobsCrawler
from core.base_crawler import BaseCrawler

CRAWLER_MAP = {
    'yahoo': YahooCrawler,
    'movies': MoviesCrawler,
    'jobs': JobsCrawler,
}

class CrawlerService:
    @staticmethod
    def get_crawler(crawler_type: str, params: Dict[str, Any]) -> BaseCrawler:
        """æ ¹æ®ç±»å‹è·å–çˆ¬è™«å®ä¾‹"""
        if crawler_type not in CRAWLER_MAP:
            raise ValueError(f"Unknown crawler: {crawler_type}")
        
        Crawler = CRAWLER_MAP[crawler_type]
        return Crawler(**params)
    
    @staticmethod
    async def run_crawler_async(crawler_type: str, params: Dict[str, Any]):
        """å¼‚æ­¥è¿è¡Œçˆ¬è™«"""
        import asyncio
        crawler = CrawlerService.get_crawler(crawler_type, params)
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œçˆ¬è™«ï¼ˆçˆ¬è™«æœ¬èº«æ˜¯åŒæ­¥çš„ï¼‰
        result = await asyncio.to_thread(crawler.run)
        return result
```

---

## ç¬¬4é˜¶æ®µï¼šä¼˜åŒ–ã€æµ‹è¯•ã€éƒ¨ç½²ï¼ˆ2å‘¨ï¼‰

**Day 33-38ï¼šæµ‹è¯•å’Œä¼˜åŒ–**

```python
# tests/integration_test.py
import pytest
from fastapi.testclient import TestClient
from backend.main import app

client = TestClient(app)

class TestCrawlerAPI:
    def test_list_crawlers(self):
        response = client.get("/api/crawlers")
        assert response.status_code == 200
        crawlers = response.json()["crawlers"]
        assert "yahoo" in crawlers

    def test_run_yahoo_crawler(self):
        response = client.post(
            "/api/crawlers/yahoo/run",
            json={"symbol": "AAPL", "days": 30}
        )
        assert response.status_code == 200
        assert "task_id" in response.json()

    def test_get_task_status(self):
        # å¯åŠ¨ä»»åŠ¡
        run_response = client.post(
            "/api/crawlers/yahoo/run",
            json={"symbol": "MSFT"}
        )
        task_id = run_response.json()["task_id"]
        
        # è·å–çŠ¶æ€
        status_response = client.get(f"/api/tasks/{task_id}")
        assert status_response.status_code == 200
        assert status_response.json()["task_id"] == task_id

    @pytest.mark.asyncio
    async def test_concurrent_crawlers(self):
        """å¹¶å‘è¿è¡Œå¤šä¸ªçˆ¬è™«"""
        import asyncio
        
        tasks = []
        for symbol in ["AAPL", "MSFT", "GOOGL"]:
            response = client.post(
                "/api/crawlers/yahoo/run",
                json={"symbol": symbol}
            )
            tasks.append(response.json()["task_id"])
        
        # éªŒè¯æ‰€æœ‰ä»»åŠ¡éƒ½å¯åŠ¨äº†
        assert len(tasks) == 3
```

æ€§èƒ½ä¼˜åŒ–ï¼š
```python
# backend/performance_tips.py

# 1. ç¼“å­˜çˆ¬è™«å®ä¾‹
from functools import lru_cache

@lru_cache(maxsize=10)
def get_cached_crawler(crawler_type: str):
    return CRAWLER_MAP[crawler_type]

# 2. ä½¿ç”¨è¿æ¥æ± 
from httpx import AsyncClient, Limits

# é…ç½®é™åˆ¶
limits = Limits(max_connections=100, max_keepalive_connections=20)
async_client = AsyncClient(limits=limits)

# 3. å¼‚æ­¥æ‰¹å¤„ç†
async def batch_crawl_stocks(symbols: list):
    """å¹¶å‘çˆ¬å–å¤šä¸ªè‚¡ç¥¨"""
    import asyncio
    tasks = [
        asyncio.create_task(fetch_stock(symbol))
        for symbol in symbols
    ]
    return await asyncio.gather(*tasks)
```

**Day 39-42ï¼šæ–‡æ¡£å’Œéƒ¨ç½²**

```markdown
# çˆ¬è™«ç®¡ç†ç³»ç»Ÿ - å®Œæ•´æŒ‡å—

## å¿«é€Ÿå¯åŠ¨

### æœ¬åœ°å¼€å‘
```bash
# 1. å¯åŠ¨åç«¯
cd backend
pip install -r requirements.txt
uvicorn main:app --reload

# 2. å¯åŠ¨å‰ç«¯ï¼ˆæ–°ç»ˆç«¯ï¼‰
cd frontend
npm install
npm start

# 3. (å¯é€‰) å¯åŠ¨Celery worker
celery -A celery_app worker --loglevel=info
```

### Dockeréƒ¨ç½²
```bash
docker-compose up
```

## APIæ–‡æ¡£
è®¿é—® http://localhost:8000/docs

## æ¶æ„è¯´æ˜
[è¯¦ç»†çš„æ¶æ„è®¾è®¡æ–‡æ¡£]

## è´¡çŒ®æŒ‡å—
[å¦‚ä½•æ·»åŠ æ–°çˆ¬è™«]
```

éƒ¨ç½²åˆ°äº‘æœåŠ¡ï¼ˆä¾‹å¦‚Renderã€Herokuï¼‰ï¼š
```yaml
# render.yaml
services:
  - type: web
    name: crawler-api
    env: python
    buildCommand: "pip install -r requirements.txt"
    startCommand: "gunicorn -w 4 -k uvicorn.workers.UvicornWorker main:app"
    
  - type: background_worker
    name: crawler-worker
    env: python
    buildCommand: "pip install -r requirements.txt"
    startCommand: "celery -A celery_app worker"
```

---

## å­¦ä¹ èµ„æºæ¸…å•

### FastAPI
- å®˜æ–¹æ–‡æ¡£ï¼šhttps://fastapi.tiangolo.com/
- æ¨èæ•™ç¨‹ï¼šhttps://www.freecodecamp.org/news/fastapi-quick-start/
- æ·±åº¦å¼‚æ­¥ï¼šhttps://realpython.com/async-io-python/

### React + TypeScript
- å®˜æ–¹æ–‡æ¡£ï¼šhttps://react.dev/
- TypeScriptæŒ‡å—ï¼šhttps://www.typescriptlang.org/docs/
- React Patternï¼šhttps://patterns.dev/posts/

### DevOps
- Dockerï¼šhttps://docs.docker.com/
- Docker Composeï¼šhttps://docs.docker.com/compose/
- Gunicorn/Uvicornï¼šhttps://gunicorn.org/

---

## é¢„æœŸå­¦ä¹ æˆæœ

âœ… æŒæ¡ç°ä»£å¼‚æ­¥Pythonå¼€å‘ï¼ˆFastAPIï¼‰
âœ… ç†è§£å‰åç«¯åˆ†ç¦»æ¶æ„
âœ… èƒ½å¤Ÿæ„å»ºç”Ÿäº§çº§REST API
âœ… ç†Ÿæ‚‰WebSocketå®æ—¶é€šä¿¡
âœ… æŒæ¡JWTè®¤è¯å’Œæˆæƒ
âœ… äº†è§£Dockerå®¹å™¨åŒ–å’Œéƒ¨ç½²
âœ… åŸºç¡€React + TypeScriptå¼€å‘
âœ… å®ç°äº†ä¸€ä¸ªå®Œæ•´çš„é¡¹ç›®ï¼ˆç®€å†äº®ç‚¹ï¼‰

## èŒä¸šä»·å€¼

- **460 Mediaé¢è¯•**ï¼šå±•ç¤ºç°ä»£å…¨æ ˆæŠ€èƒ½
- **æŠ€æœ¯é¢**ï¼šèƒ½æ·±å…¥è®¨è®ºå¼‚æ­¥ã€æ€§èƒ½ä¼˜åŒ–ã€ç³»ç»Ÿè®¾è®¡
- **GitHub**ï¼šæœ‰ä¸€ä¸ªå®Œæ•´é¡¹ç›®ä½œä¸ºä½œå“é›†
- **æœªæ¥å‡çº§**ï¼šå¯è½»æ¾æ‰©å±•åˆ°Kubernetesã€å¾®æœåŠ¡ç­‰

---

## å¸¸è§é—®é¢˜

**Q: 2ä¸ªæœˆå¤Ÿä¸å¤Ÿï¼Ÿ**
A: å®Œå…¨å¤Ÿã€‚ç¬¬1å‘¨æŒæ¡åŸºç¡€ï¼Œç¬¬2å‘¨é›†æˆçˆ¬è™«ï¼Œç¬¬3-4å‘¨ä¼˜åŒ–å’Œéƒ¨ç½²ã€‚å¦‚æœéœ€è¦åŠ å¿«ï¼Œå¯è·³è¿‡éƒ¨åˆ†Reacté«˜çº§ç‰¹æ€§ã€‚

**Q: éœ€è¦å…ˆå­¦ä¼šReactå—ï¼Ÿ**
A: ä¸éœ€è¦ã€‚å¯ä»¥ç”¨ç®€å•çš„HTML+JavaScriptæ›¿ä»£Reactï¼Œæˆ–å‚è€ƒæˆ‘æä¾›çš„ä»£ç ã€‚

**Q: éƒ¨ç½²å¾ˆéš¾å—ï¼Ÿ**
A: ä¸éš¾ã€‚Docker-Composeä¸€é”®å¯åŠ¨ï¼Œéƒ¨ç½²åˆ°Render/AWSåªéœ€æ”¹å‡ ä¸ªé…ç½®ã€‚

**Q: çˆ¬è™«çš„æ”¹é€ æˆæœ¬é«˜å—ï¼Ÿ**
A: ä½ã€‚åªéœ€åœ¨çˆ¬è™«å¤–åŒ…è£…ä¸€ä¸ªå¼‚æ­¥æ¥å£ï¼Œçˆ¬è™«æœ¬èº«ä¸éœ€æ”¹åŠ¨ã€‚

---

## æ—¶é—´è¡¨æ€»ç»“

| å‘¨ | ä»»åŠ¡ | æ—¶é—´æŠ•å…¥ |
|----|------|--------|
| 1-2 | FastAPI + å¼‚æ­¥ + çˆ¬è™«é›†æˆ | 60å°æ—¶ |
| 3-4 | ç”Ÿäº§çº§éƒ¨ç½² + æ•°æ®åº“ | 50å°æ—¶ |
| 5 | Reactå‰ç«¯å¼€å‘ | 40å°æ—¶ |
| 6 | é›†æˆ + ä¼˜åŒ– | 35å°æ—¶ |
| 7-8 | æµ‹è¯• + æ–‡æ¡£ + éƒ¨ç½² | 35å°æ—¶ |
| **æ€»è®¡** | | **220å°æ—¶** |

**æŒ‰æ¯å¤©8å°æ—¶å­¦ä¹ è®¡ç®— = 27.5å¤© = ~4å‘¨**

æ‰€ä»¥2ä¸ªæœˆè¶³å¤Ÿæœ‰ä½™ï¼Œè¿˜èƒ½æ‰“ç£¨ç»†èŠ‚ã€‚

---

**ç°åœ¨å°±å¼€å§‹å§ï¼** ğŸš€
